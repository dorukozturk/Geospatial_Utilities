- hosts: localhost
  connection: local
  tasks:
    # Find the Master AMI if master_instance_ami is not specified

    - name: Find latest Master AMI
      ec2_ami_find:
        state: available
        sort: creationDate
        sort_order: descending
        ami_tags:
          ec2_group: "{{ prefix }}"
          ec2_type: master
        region: "{{ ec2_region }}"
      register: master_amis
      tags:
        - master

    - name: Register to master_instance_ami variable
      set_fact:
        master_ami: "{{ master_amis.results[0]['ami_id'] }}"
      tags:
        - master


    - name: Launch master instance
      ec2:
        # Required variables
        instance_type: "{{ ec2_master_instance_type }}"
        image: "{{ master_ami }}"
        # There can be only one...
        exact_count: 1
        region: "{{ ec2_region }}"

        # Optional variables
        group: "{{ ec2_security_group|default(omit) }}"
        key_name: "{{ ec2_key_name|default(omit) }}"
        instance_profile_name: "{{ ec2_instance_profile_name|default(omit) }}"
        user_data: "{{ lookup('template', 'scripts/init_master.sh') }}"
        volumes: "{{ ec2_master_volumes|default(omit) }}"
        # Derived or static variables
        instance_tags: "{{ master_tag|combine(ec2_master_additional_tags|default({})) }}"
        count_tag: "{{ master_tag }}"
        wait: yes
      register: master


    - name: Poll instance data to get public DNS names
      ec2_remote_facts:
        filters:
          instance-id: >-
            {{ master.tagged_instances | map(attribute='id') | list }}
        region: "{{ ec2_region }}"
      register: instances

    - name: Wait for SSH to come up on Master
      wait_for:
        host: "{{ item.public_dns_name }}"
        port: 22
        timeout: "{{ ec2_launch_instance_timeout }}"
        state: started
      with_items: "{{ instances.instances }}"

    - name: Find latest Node AMI
      ec2_ami_find:
        state: available
        sort: creationDate
        sort_order: descending
        ami_tags:
          ec2_group: "{{ prefix }}"
          ec2_type: node
        region: "{{ ec2_region }}"
      register: node_amis


    - name: Register to master_instance_ami variable
      set_fact:
        node_ami: "{{ node_amis.results[0]['ami_id'] }}"

    - set_fact:
        master_ip: "{{ master.tagged_instances[0]['private_ip'] }}"

    - name: Launch node instances
      ec2:
        # Required variables
        instance_type: "{{ ec2_node_instance_type }}"
        image: "{{ node_ami }}"
        exact_count: "{{ ec2_node_count }}"
        region: "{{ ec2_region }}"

        # Optional variables
        group: "{{ ec2_security_group|default(omit) }}"
        key_name: "{{ ec2_key_name|default(omit) }}"
        instance_profile_name: "{{ ec2_instance_profile_name|default(omit) }}"
        volumes: "{{ ec2_node_volumes|default(omit) }}"
        user_data: "{{ lookup('template', 'scripts/init_node.sh') }}"
        # Derived or static variables
        instance_tags: "{{ node_tag|combine(ec2_node_additional_tags|default({})) }}"
        count_tag: "{{ node_tag }}"
      register: nodes

# Could potentially restart flower etc here to clear out 'offline' workers when we
# Change the node count. This would allow us to simulate a poor man's scaling group
